{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAEs with nnsight\n",
    "Short demo on loading SAEs and caching feature activations with nnsight\n",
    "\n",
    "- Refer to this repo for more info basic usage, stats, and downloads for Sam's pythia-70m SAEs: https://github.com/saprmarks/dictionary_learning\n",
    "- They're also available on my gdrive: https://drive.google.com/drive/folders/14fPh8gf16bLGJ0MD1vAyy6bog1lr2dSf?usp=sharing\n",
    "- Here's an example for using Sam's SAEs in a google colab: https://colab.research.google.com/drive/1C9RjyB8Tia4CY9UOZVX4o3MmxdDirrqD?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dictionary_learning import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "DEBUGGING = True\n",
    "D_MODEL = 512\n",
    "D_SAE = 32768\n",
    "DICT_ID = 10\n",
    "DICT_PATH = \"/share/projects/dictionary_circuits/autoencoders\"\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = {'validate' : True, 'scan' : True}\n",
    "else:\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = LanguageModel(\n",
    "    \"EleutherAI/pythia-70m-deduped\",\n",
    "    device_map = DEVICE,\n",
    "    dispatch = True,\n",
    ")\n",
    "\n",
    "# Load submodules and dictionaries\n",
    "embed = model.gpt_neox.embed_in\n",
    "attns = [layer.attention for layer in model.gpt_neox.layers]\n",
    "mlps = [layer.mlp for layer in model.gpt_neox.layers]\n",
    "resids = [layer for layer in model.gpt_neox.layers]\n",
    "submodules = attns + mlps + resids + [embed]\n",
    "\n",
    "dictionaries = {}\n",
    "submodule_to_name = {}\n",
    "ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "ae.load_state_dict(t.load(f'{DICT_PATH}/pythia-70m-deduped/embed/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "dictionaries[embed] = ae\n",
    "submodule_to_name[embed] = 'embed'\n",
    "for i in range(len(model.gpt_neox.layers)):\n",
    "    ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'{DICT_PATH}/pythia-70m-deduped/attn_out_layer{i}/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "    dictionaries[attns[i]] = ae\n",
    "    submodule_to_name[attns[i]] = f'attn{i}'\n",
    "\n",
    "    ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'{DICT_PATH}/pythia-70m-deduped/mlp_out_layer{i}/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "    dictionaries[mlps[i]] = ae\n",
    "    submodule_to_name[mlps[i]] = f'mlp{i}'\n",
    "\n",
    "    ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'{DICT_PATH}/pythia-70m-deduped/resid_out_layer{i}/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "    dictionaries[resids[i]] = ae\n",
    "    submodule_to_name[resids[i]] = f'resid{i}'\n",
    "\n",
    "name_to_submodule = {v: k for k, v in submodule_to_name.items()}\n",
    "\n",
    "# Run through a test input to figure out which hidden states are tuples\n",
    "is_tuple = {}\n",
    "with model.trace(\"_\"):\n",
    "    for submodule in submodules:\n",
    "        is_tuple[submodule] = type(submodule.output.shape) == tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache feature activations with nnsight\n",
    "prompt = \"Simple input\"\n",
    "\n",
    "feature_activations = {}\n",
    "reconstruction_errors = {}\n",
    "\n",
    "with t.no_grad(), model.trace(prompt, **tracer_kwargs):\n",
    "    for submodule in submodules:\n",
    "        x = submodule.output\n",
    "        if is_tuple[submodule]:\n",
    "            x = x[0]\n",
    "\n",
    "        x_hat, f = dictionaries[submodule].forward(x, output_features=True)\n",
    "        x_err = x - x_hat\n",
    "        \n",
    "        feature_activations[submodule] = f.save()\n",
    "        reconstruction_errors[submodule] = x_err.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 32768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect cached feature activation\n",
    "name = \"resid0\"\n",
    "feature_activations[name_to_submodule[name]].shape # (batch_size, seq_len, hidden_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
